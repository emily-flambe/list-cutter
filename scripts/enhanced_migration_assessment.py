#!/usr/bin/env python3
"""
Enhanced Migration Assessment for Issue #66
==========================================

Advanced file migration assessment with comprehensive analysis, risk evaluation,
and production-ready migration planning capabilities.

Features:
- Comprehensive file system analysis with performance profiling
- Advanced risk assessment and mitigation planning
- Cost analysis with detailed projections
- Migration strategy optimization
- Infrastructure capacity planning
- Timeline estimation with confidence intervals
- Dependency analysis and prerequisite validation

Author: Claude Code
Version: 1.0.0
"""

import asyncio
import json
import logging
import os
import sys
import time
import hashlib
import mimetypes
import statistics
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field, asdict
from datetime import datetime, timezone, timedelta
from enum import Enum
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Set
from collections import defaultdict, Counter
import sqlite3

import click
import requests
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, MofNCompleteColumn
from rich.tree import Tree
from rich.panel import Panel
from rich.prompt import Confirm
import numpy as np

console = Console()

class RiskLevel(Enum):
    """Risk assessment levels"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class MigrationStrategy(Enum):
    """Migration strategy types"""
    BIG_BANG = "big_bang"
    PHASED = "phased"
    PARALLEL = "parallel"
    GRADUAL = "gradual"
    PILOT = "pilot"

class FileCategory(Enum):
    """File categorization for migration planning"""
    CRITICAL = "critical"
    IMPORTANT = "important"
    STANDARD = "standard"
    ARCHIVE = "archive"
    TEMPORARY = "temporary"

@dataclass
class FileAnalysis:
    """Detailed file analysis results"""
    path: str
    size: int
    modified: datetime
    accessed: datetime
    created: datetime
    mime_type: Optional[str]
    extension: str
    checksum: Optional[str] = None
    
    # Classification
    category: FileCategory = FileCategory.STANDARD
    priority: int = 5  # 1-10 scale
    
    # Risk factors
    is_corrupted: bool = False
    is_duplicate: bool = False
    has_access_issues: bool = False
    is_large_file: bool = False
    is_frequently_accessed: bool = False
    
    # Performance metrics
    read_time: float = 0.0
    checksum_time: float = 0.0
    
    def __post_init__(self):
        if isinstance(self.modified, str):
            self.modified = datetime.fromisoformat(self.modified)
        if isinstance(self.accessed, str):
            self.accessed = datetime.fromisoformat(self.accessed)
        if isinstance(self.created, str):
            self.created = datetime.fromisoformat(self.created)

@dataclass
class MigrationBatch:
    """Enhanced migration batch with detailed planning"""
    id: str
    name: str
    description: str
    files: List[FileAnalysis]
    total_size: int
    total_files: int
    
    # Planning
    strategy: MigrationStrategy
    priority: int
    estimated_duration: float  # hours
    estimated_cost: float
    
    # Risk assessment
    risk_level: RiskLevel
    risk_factors: List[str] = field(default_factory=list)
    mitigation_strategies: List[str] = field(default_factory=list)
    
    # Dependencies
    prerequisites: List[str] = field(default_factory=list)
    dependencies: List[str] = field(default_factory=list)
    
    # Resources
    required_bandwidth: float = 0.0  # Mbps
    required_storage: float = 0.0  # GB
    required_workers: int = 1
    
    def risk_score(self) -> float:
        """Calculate overall risk score"""
        base_score = {
            RiskLevel.LOW: 1,
            RiskLevel.MEDIUM: 3,
            RiskLevel.HIGH: 7,
            RiskLevel.CRITICAL: 10
        }[self.risk_level]
        
        # Adjust for batch size and complexity
        size_factor = min(self.total_size / (1024**3), 10) * 0.1  # GB factor
        file_factor = min(self.total_files / 10000, 10) * 0.1  # File count factor
        
        return min(base_score + size_factor + file_factor, 10)

@dataclass
class InfrastructureRequirements:
    """Infrastructure requirements analysis"""
    # Network requirements
    peak_bandwidth_mbps: float
    sustained_bandwidth_mbps: float
    total_transfer_gb: float
    
    # Storage requirements
    source_storage_gb: float
    target_storage_gb: float
    temporary_storage_gb: float
    
    # Compute requirements
    recommended_workers: int
    memory_per_worker_mb: int
    cpu_cores_recommended: int
    
    # Timeline
    estimated_duration_hours: float
    confidence_interval: Tuple[float, float]
    
    # Cost analysis
    infrastructure_cost: float
    operational_cost: float
    total_cost: float

@dataclass
class RiskAssessment:
    """Comprehensive risk assessment"""
    overall_risk: RiskLevel
    risk_score: float
    
    # Risk categories
    data_integrity_risk: RiskLevel
    availability_risk: RiskLevel
    performance_risk: RiskLevel
    security_risk: RiskLevel
    operational_risk: RiskLevel
    
    # Specific risks
    identified_risks: List[Dict[str, Any]] = field(default_factory=list)
    mitigation_strategies: List[Dict[str, Any]] = field(default_factory=list)
    
    # Impact assessment
    worst_case_scenario: str = ""
    recovery_time_estimate: float = 0.0  # hours
    business_impact: str = ""

@dataclass
class PerformanceProfile:
    """Performance profiling results"""
    avg_read_speed_mbps: float
    peak_read_speed_mbps: float
    avg_file_processing_time: float
    large_file_processing_time: float
    
    # I/O patterns
    sequential_read_performance: float
    random_read_performance: float
    concurrent_read_performance: float
    
    # Bottlenecks
    identified_bottlenecks: List[str] = field(default_factory=list)
    optimization_recommendations: List[str] = field(default_factory=list)

@dataclass
class EnhancedAssessmentReport:
    """Comprehensive enhanced assessment report"""
    assessment_id: str
    timestamp: datetime
    
    # Basic statistics
    total_files: int
    total_size: int
    analyzed_files: int
    
    # File analysis
    file_categories: Dict[FileCategory, int]
    file_types: Dict[str, int]
    size_distribution: Dict[str, int]
    access_patterns: Dict[str, int]
    
    # Risk assessment
    risk_assessment: RiskAssessment
    
    # Performance analysis
    performance_profile: PerformanceProfile
    
    # Migration planning
    recommended_strategy: MigrationStrategy
    migration_batches: List[MigrationBatch]
    infrastructure_requirements: InfrastructureRequirements
    
    # Quality metrics
    data_quality_score: float
    readiness_score: float
    complexity_score: float
    
    # Recommendations
    recommendations: List[str] = field(default_factory=list)
    action_items: List[str] = field(default_factory=list)
    
    def overall_readiness(self) -> str:
        """Determine overall migration readiness"""
        if self.readiness_score >= 90:
            return "Ready"
        elif self.readiness_score >= 70:
            return "Mostly Ready"
        elif self.readiness_score >= 50:
            return "Needs Preparation"
        else:
            return "Not Ready"

class EnhancedFileAnalyzer:
    """Advanced file analysis with performance profiling"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.performance_samples = []
        
    async def analyze_file_system(self, django_media_root: str) -> Dict[str, Any]:
        """Comprehensive file system analysis"""
        console.print("[blue]ðŸ” Starting comprehensive file system analysis...[/blue]")
        
        media_path = Path(django_media_root)
        if not media_path.exists():
            raise FileNotFoundError(f"Media root not found: {django_media_root}")
        
        # Initialize analysis results
        analysis_results = {
            'files': [],
            'performance_data': [],
            'access_patterns': defaultdict(int),
            'file_categories': defaultdict(int),
            'quality_issues': [],
            'duplicates': [],
            'large_files': []
        }
        
        # Get all files
        all_files = list(media_path.rglob('*'))
        file_paths = [f for f in all_files if f.is_file()]
        
        console.print(f"[yellow]Found {len(file_paths)} files to analyze[/yellow]")
        
        # Analyze files with progress tracking
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            MofNCompleteColumn(),
            console=console
        ) as progress:
            
            task = progress.add_task("Analyzing files...", total=len(file_paths))
            
            # Process files in batches for performance
            batch_size = self.config.get('batch_size', 100)
            
            with ThreadPoolExecutor(max_workers=self.config.get('max_workers', 10)) as executor:
                futures = []
                
                for i in range(0, len(file_paths), batch_size):
                    batch = file_paths[i:i + batch_size]
                    future = executor.submit(self._analyze_file_batch, batch, django_media_root)
                    futures.append(future)
                
                # Collect results
                for future in as_completed(futures):
                    try:
                        batch_results = future.result()
                        analysis_results['files'].extend(batch_results['files'])
                        analysis_results['performance_data'].extend(batch_results['performance_data'])
                        
                        # Update progress
                        progress.update(task, advance=len(batch_results['files']))
                        
                    except Exception as e:
                        console.print(f"[red]Error analyzing file batch: {e}[/red]")
                        progress.update(task, advance=batch_size)
        
        # Post-process analysis results
        await self._post_process_analysis(analysis_results)
        
        console.print(f"[green]âœ… Analysis completed: {len(analysis_results['files'])} files analyzed[/green]")
        return analysis_results
    
    def _analyze_file_batch(self, file_paths: List[Path], media_root: str) -> Dict[str, Any]:
        """Analyze a batch of files"""
        batch_results = {
            'files': [],
            'performance_data': []
        }
        
        for file_path in file_paths:
            try:
                file_analysis = self._analyze_single_file(file_path, media_root)
                batch_results['files'].append(file_analysis)
                
                # Record performance data
                batch_results['performance_data'].append({
                    'file_size': file_analysis.size,
                    'read_time': file_analysis.read_time,
                    'checksum_time': file_analysis.checksum_time
                })
                
            except Exception as e:
                # Log error but continue processing
                continue
        
        return batch_results
    
    def _analyze_single_file(self, file_path: Path, media_root: str) -> FileAnalysis:
        """Analyze a single file"""
        start_time = time.time()
        
        # Get file stats
        stat = file_path.stat()
        
        # Basic file info
        analysis = FileAnalysis(
            path=str(file_path.relative_to(media_root)),
            size=stat.st_size,
            modified=datetime.fromtimestamp(stat.st_mtime, tz=timezone.utc),
            accessed=datetime.fromtimestamp(stat.st_atime, tz=timezone.utc),
            created=datetime.fromtimestamp(stat.st_ctime, tz=timezone.utc),
            mime_type=mimetypes.guess_type(str(file_path))[0],
            extension=file_path.suffix.lower()
        )
        
        # Performance testing - read file
        read_start = time.time()
        try:
            with open(file_path, 'rb') as f:
                # Read first chunk to test read performance
                chunk = f.read(min(1024 * 1024, analysis.size))  # 1MB or file size
            analysis.read_time = time.time() - read_start
        except Exception as e:
            analysis.has_access_issues = True
            analysis.read_time = 0.0
        
        # Calculate checksum for integrity checking
        if self.config.get('calculate_checksums', False) and analysis.size < 100 * 1024 * 1024:  # Only for files < 100MB
            checksum_start = time.time()
            analysis.checksum = self._calculate_checksum(file_path)
            analysis.checksum_time = time.time() - checksum_start
        
        # File categorization
        analysis.category = self._categorize_file(analysis)
        analysis.priority = self._calculate_priority(analysis)
        
        # Risk factor assessment
        self._assess_file_risks(analysis)
        
        return analysis
    
    def _calculate_checksum(self, file_path: Path) -> Optional[str]:
        """Calculate SHA-256 checksum"""
        try:
            hash_sha256 = hashlib.sha256()
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_sha256.update(chunk)
            return hash_sha256.hexdigest()
        except Exception:
            return None
    
    def _categorize_file(self, analysis: FileAnalysis) -> FileCategory:
        """Categorize file based on characteristics"""
        # Age-based categorization
        age_days = (datetime.now(timezone.utc) - analysis.modified).days
        
        if age_days > 365:
            return FileCategory.ARCHIVE
        elif analysis.extension in ['.tmp', '.temp', '.cache']:
            return FileCategory.TEMPORARY
        elif analysis.size > 100 * 1024 * 1024:  # > 100MB
            return FileCategory.CRITICAL
        elif analysis.mime_type and analysis.mime_type.startswith('image/'):
            return FileCategory.IMPORTANT
        else:
            return FileCategory.STANDARD
    
    def _calculate_priority(self, analysis: FileAnalysis) -> int:
        """Calculate migration priority (1-10)"""
        priority = 5  # Base priority
        
        # Adjust based on category
        category_adjustments = {
            FileCategory.CRITICAL: 2,
            FileCategory.IMPORTANT: 1,
            FileCategory.STANDARD: 0,
            FileCategory.ARCHIVE: -2,
            FileCategory.TEMPORARY: -3
        }
        priority += category_adjustments[analysis.category]
        
        # Adjust based on size
        if analysis.size > 1024 * 1024 * 1024:  # > 1GB
            priority += 2
        elif analysis.size < 1024:  # < 1KB
            priority -= 1
        
        # Adjust based on access recency
        days_since_access = (datetime.now(timezone.utc) - analysis.accessed).days
        if days_since_access < 30:
            priority += 1
        elif days_since_access > 365:
            priority -= 1
        
        return max(1, min(10, priority))
    
    def _assess_file_risks(self, analysis: FileAnalysis):
        """Assess risk factors for file"""
        # Large file risk
        if analysis.size > 1024 * 1024 * 1024:  # > 1GB
            analysis.is_large_file = True
        
        # Recent access pattern
        days_since_access = (datetime.now(timezone.utc) - analysis.accessed).days
        if days_since_access < 7:
            analysis.is_frequently_accessed = True
        
        # File corruption check (basic)
        if analysis.has_access_issues or analysis.read_time == 0:
            analysis.is_corrupted = True
    
    async def _post_process_analysis(self, analysis_results: Dict[str, Any]):
        """Post-process analysis results for duplicates and patterns"""
        files = analysis_results['files']
        
        # Find duplicates by checksum
        checksum_map = defaultdict(list)
        for file_analysis in files:
            if file_analysis.checksum:
                checksum_map[file_analysis.checksum].append(file_analysis)
        
        # Mark duplicates
        for checksum, file_list in checksum_map.items():
            if len(file_list) > 1:
                for file_analysis in file_list[1:]:  # Keep first as original
                    file_analysis.is_duplicate = True
                analysis_results['duplicates'].extend(file_list)
        
        # Identify large files
        analysis_results['large_files'] = [
            f for f in files if f.size > 100 * 1024 * 1024
        ]
        
        # Access pattern analysis
        for file_analysis in files:
            days_since_access = (datetime.now(timezone.utc) - file_analysis.accessed).days
            if days_since_access < 7:
                analysis_results['access_patterns']['recent'] += 1
            elif days_since_access < 30:
                analysis_results['access_patterns']['monthly'] += 1
            elif days_since_access < 365:
                analysis_results['access_patterns']['yearly'] += 1
            else:
                analysis_results['access_patterns']['old'] += 1

class MigrationPlanner:
    """Advanced migration planning with optimization"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        
    def generate_migration_plan(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """Generate optimized migration plan"""
        console.print("[blue]ðŸ“‹ Generating optimized migration plan...[/blue]")
        
        files = analysis_results['files']
        
        # Strategy selection
        recommended_strategy = self._select_migration_strategy(files)
        
        # Create migration batches
        batches = self._create_optimized_batches(files, recommended_strategy)
        
        # Infrastructure requirements
        infrastructure_reqs = self._calculate_infrastructure_requirements(files, batches)
        
        # Risk assessment
        risk_assessment = self._assess_migration_risks(files, batches)
        
        return {
            'strategy': recommended_strategy,
            'batches': batches,
            'infrastructure_requirements': infrastructure_reqs,
            'risk_assessment': risk_assessment,
            'total_estimated_duration': sum(b.estimated_duration for b in batches),
            'total_estimated_cost': sum(b.estimated_cost for b in batches)
        }
    
    def _select_migration_strategy(self, files: List[FileAnalysis]) -> MigrationStrategy:
        """Select optimal migration strategy"""
        total_size = sum(f.size for f in files)
        total_files = len(files)
        critical_files = sum(1 for f in files if f.category == FileCategory.CRITICAL)
        
        # Decision logic
        if total_size > 10 * 1024**3:  # > 10TB
            return MigrationStrategy.PHASED
        elif critical_files > total_files * 0.3:  # > 30% critical
            return MigrationStrategy.GRADUAL
        elif total_files > 100000:  # > 100k files
            return MigrationStrategy.PARALLEL
        else:
            return MigrationStrategy.BIG_BANG
    
    def _create_optimized_batches(self, files: List[FileAnalysis], 
                                 strategy: MigrationStrategy) -> List[MigrationBatch]:
        """Create optimized migration batches"""
        batches = []
        
        # Sort files by priority and category
        sorted_files = sorted(files, key=lambda f: (f.category.value, -f.priority, -f.size))
        
        # Group by strategy
        if strategy == MigrationStrategy.PHASED:
            batches = self._create_phased_batches(sorted_files)
        elif strategy == MigrationStrategy.PARALLEL:
            batches = self._create_parallel_batches(sorted_files)
        elif strategy == MigrationStrategy.GRADUAL:
            batches = self._create_gradual_batches(sorted_files)
        else:
            # Big bang - single batch
            batch = self._create_single_batch(sorted_files, "Complete Migration", strategy)
            batches = [batch]
        
        # Optimize batch order
        batches = self._optimize_batch_order(batches)
        
        return batches
    
    def _create_phased_batches(self, files: List[FileAnalysis]) -> List[MigrationBatch]:
        """Create phased migration batches"""
        batches = []
        
        # Phase 1: Critical and important files
        phase1_files = [f for f in files if f.category in [FileCategory.CRITICAL, FileCategory.IMPORTANT]]
        if phase1_files:
            batch = self._create_single_batch(phase1_files, "Phase 1: Critical Files", MigrationStrategy.PHASED)
            batch.priority = 10
            batches.append(batch)
        
        # Phase 2: Standard files
        phase2_files = [f for f in files if f.category == FileCategory.STANDARD]
        if phase2_files:
            # Split into smaller batches if too large
            for i, batch_files in enumerate(self._split_into_batches(phase2_files, max_size_gb=10)):
                batch = self._create_single_batch(batch_files, f"Phase 2: Standard Files (Batch {i+1})", MigrationStrategy.PHASED)
                batch.priority = 7
                batches.append(batch)
        
        # Phase 3: Archive files
        phase3_files = [f for f in files if f.category == FileCategory.ARCHIVE]
        if phase3_files:
            batch = self._create_single_batch(phase3_files, "Phase 3: Archive Files", MigrationStrategy.PHASED)
            batch.priority = 3
            batches.append(batch)
        
        return batches
    
    def _create_parallel_batches(self, files: List[FileAnalysis]) -> List[MigrationBatch]:
        """Create parallel migration batches"""
        batches = []
        
        # Split files into equal-sized batches for parallel processing
        max_workers = self.config.get('max_parallel_batches', 5)
        batch_size = len(files) // max_workers
        
        for i in range(max_workers):
            start_idx = i * batch_size
            end_idx = start_idx + batch_size if i < max_workers - 1 else len(files)
            batch_files = files[start_idx:end_idx]
            
            if batch_files:
                batch = self._create_single_batch(batch_files, f"Parallel Batch {i+1}", MigrationStrategy.PARALLEL)
                batch.priority = 8
                batches.append(batch)
        
        return batches
    
    def _create_gradual_batches(self, files: List[FileAnalysis]) -> List[MigrationBatch]:
        """Create gradual migration batches"""
        batches = []
        
        # Small incremental batches
        max_batch_size_gb = 2  # 2GB per batch for gradual migration
        
        for i, batch_files in enumerate(self._split_into_batches(files, max_size_gb=max_batch_size_gb)):
            batch = self._create_single_batch(batch_files, f"Gradual Migration Batch {i+1}", MigrationStrategy.GRADUAL)
            batch.priority = 6
            batches.append(batch)
        
        return batches
    
    def _create_single_batch(self, files: List[FileAnalysis], name: str, 
                           strategy: MigrationStrategy) -> MigrationBatch:
        """Create a single migration batch"""
        total_size = sum(f.size for f in files)
        
        batch = MigrationBatch(
            id=f"batch_{int(time.time())}_{len(files)}",
            name=name,
            description=f"Migration batch with {len(files)} files ({self._format_size(total_size)})",
            files=files,
            total_size=total_size,
            total_files=len(files),
            strategy=strategy,
            priority=5,
            estimated_duration=self._estimate_batch_duration(files),
            estimated_cost=self._estimate_batch_cost(files),
            risk_level=self._assess_batch_risk(files)
        )
        
        # Add risk factors and mitigation strategies
        self._populate_batch_risks(batch)
        
        return batch
    
    def _split_into_batches(self, files: List[FileAnalysis], max_size_gb: float) -> List[List[FileAnalysis]]:
        """Split files into batches based on size limit"""
        batches = []
        current_batch = []
        current_size = 0
        max_size_bytes = max_size_gb * 1024**3
        
        for file in files:
            if current_size + file.size > max_size_bytes and current_batch:
                batches.append(current_batch)
                current_batch = [file]
                current_size = file.size
            else:
                current_batch.append(file)
                current_size += file.size
        
        if current_batch:
            batches.append(current_batch)
        
        return batches
    
    def _estimate_batch_duration(self, files: List[FileAnalysis]) -> float:
        """Estimate batch migration duration in hours"""
        total_size = sum(f.size for f in files)
        
        # Base transfer rate: 10 MB/s
        base_transfer_rate = 10 * 1024 * 1024  # bytes/second
        
        # Adjust for file count overhead
        file_overhead = len(files) * 0.1  # 0.1 seconds per file
        
        # Transfer time
        transfer_time = total_size / base_transfer_rate + file_overhead
        
        # Add overhead for processing, validation, etc.
        total_time = transfer_time * 1.5  # 50% overhead
        
        return total_time / 3600  # Convert to hours
    
    def _estimate_batch_cost(self, files: List[FileAnalysis]) -> float:
        """Estimate batch migration cost in USD"""
        total_size = sum(f.size for f in files)
        
        # R2 storage cost: $0.015 per GB per month
        storage_cost = (total_size / 1024**3) * 0.015
        
        # Operation cost: $0.36 per 1M PUT operations
        operation_cost = (len(files) / 1_000_000) * 0.36
        
        # Processing cost (estimated)
        processing_cost = (total_size / 1024**3) * 0.001
        
        return storage_cost + operation_cost + processing_cost
    
    def _assess_batch_risk(self, files: List[FileAnalysis]) -> RiskLevel:
        """Assess risk level for batch"""
        risk_factors = 0
        
        # Large files increase risk
        large_files = sum(1 for f in files if f.size > 1024**3)
        risk_factors += large_files * 0.5
        
        # Corrupted files increase risk
        corrupted_files = sum(1 for f in files if f.is_corrupted)
        risk_factors += corrupted_files * 2
        
        # Critical files increase risk
        critical_files = sum(1 for f in files if f.category == FileCategory.CRITICAL)
        risk_factors += critical_files * 0.3
        
        # Determine risk level
        if risk_factors >= 10:
            return RiskLevel.CRITICAL
        elif risk_factors >= 5:
            return RiskLevel.HIGH
        elif risk_factors >= 2:
            return RiskLevel.MEDIUM
        else:
            return RiskLevel.LOW
    
    def _populate_batch_risks(self, batch: MigrationBatch):
        """Populate batch risk factors and mitigation strategies"""
        # Risk factors
        if any(f.is_large_file for f in batch.files):
            batch.risk_factors.append("Contains large files (>1GB)")
            batch.mitigation_strategies.append("Use parallel processing for large files")
        
        if any(f.is_corrupted for f in batch.files):
            batch.risk_factors.append("Contains potentially corrupted files")
            batch.mitigation_strategies.append("Verify file integrity before migration")
        
        if batch.total_size > 100 * 1024**3:  # > 100GB
            batch.risk_factors.append("Large batch size")
            batch.mitigation_strategies.append("Consider splitting into smaller batches")
        
        # Prerequisites
        batch.prerequisites.extend([
            "Verify network connectivity",
            "Confirm R2 storage quota",
            "Validate file permissions"
        ])
    
    def _optimize_batch_order(self, batches: List[MigrationBatch]) -> List[MigrationBatch]:
        """Optimize batch execution order"""
        # Sort by priority (high to low) and risk (low to high)
        return sorted(batches, key=lambda b: (-b.priority, b.risk_score()))
    
    def _calculate_infrastructure_requirements(self, files: List[FileAnalysis], 
                                             batches: List[MigrationBatch]) -> InfrastructureRequirements:
        """Calculate infrastructure requirements"""
        total_size = sum(f.size for f in files)
        total_size_gb = total_size / 1024**3
        
        # Network requirements
        peak_bandwidth = 100  # Mbps
        sustained_bandwidth = 50  # Mbps
        
        # Storage requirements
        source_storage = total_size_gb
        target_storage = total_size_gb * 1.1  # 10% overhead
        temporary_storage = total_size_gb * 0.2  # 20% for temporary files
        
        # Compute requirements
        recommended_workers = min(len(batches), 10)
        memory_per_worker = 512  # MB
        cpu_cores = recommended_workers * 2
        
        # Timeline estimation
        estimated_hours = sum(b.estimated_duration for b in batches)
        confidence_interval = (estimated_hours * 0.8, estimated_hours * 1.5)
        
        # Cost calculation
        infrastructure_cost = estimated_hours * 0.5  # $0.5 per hour
        operational_cost = sum(b.estimated_cost for b in batches)
        total_cost = infrastructure_cost + operational_cost
        
        return InfrastructureRequirements(
            peak_bandwidth_mbps=peak_bandwidth,
            sustained_bandwidth_mbps=sustained_bandwidth,
            total_transfer_gb=total_size_gb,
            source_storage_gb=source_storage,
            target_storage_gb=target_storage,
            temporary_storage_gb=temporary_storage,
            recommended_workers=recommended_workers,
            memory_per_worker_mb=memory_per_worker,
            cpu_cores_recommended=cpu_cores,
            estimated_duration_hours=estimated_hours,
            confidence_interval=confidence_interval,
            infrastructure_cost=infrastructure_cost,
            operational_cost=operational_cost,
            total_cost=total_cost
        )
    
    def _assess_migration_risks(self, files: List[FileAnalysis], 
                              batches: List[MigrationBatch]) -> RiskAssessment:
        """Comprehensive risk assessment"""
        # Calculate overall risk score
        batch_risk_scores = [b.risk_score() for b in batches]
        overall_risk_score = np.mean(batch_risk_scores) if batch_risk_scores else 0
        
        # Determine overall risk level
        if overall_risk_score >= 7:
            overall_risk = RiskLevel.CRITICAL
        elif overall_risk_score >= 5:
            overall_risk = RiskLevel.HIGH
        elif overall_risk_score >= 3:
            overall_risk = RiskLevel.MEDIUM
        else:
            overall_risk = RiskLevel.LOW
        
        # Specific risk assessments
        data_integrity_risk = self._assess_data_integrity_risk(files)
        availability_risk = self._assess_availability_risk(files, batches)
        performance_risk = self._assess_performance_risk(files)
        security_risk = self._assess_security_risk(files)
        operational_risk = self._assess_operational_risk(batches)
        
        # Identified risks
        identified_risks = self._identify_specific_risks(files, batches)
        mitigation_strategies = self._generate_mitigation_strategies(identified_risks)
        
        return RiskAssessment(
            overall_risk=overall_risk,
            risk_score=overall_risk_score,
            data_integrity_risk=data_integrity_risk,
            availability_risk=availability_risk,
            performance_risk=performance_risk,
            security_risk=security_risk,
            operational_risk=operational_risk,
            identified_risks=identified_risks,
            mitigation_strategies=mitigation_strategies,
            worst_case_scenario="Complete migration failure requiring full rollback",
            recovery_time_estimate=24.0,  # 24 hours
            business_impact="Temporary file access disruption during rollback"
        )
    
    def _assess_data_integrity_risk(self, files: List[FileAnalysis]) -> RiskLevel:
        """Assess data integrity risk"""
        corrupted_files = sum(1 for f in files if f.is_corrupted)
        corruption_rate = corrupted_files / len(files) if files else 0
        
        if corruption_rate > 0.05:  # > 5%
            return RiskLevel.HIGH
        elif corruption_rate > 0.01:  # > 1%
            return RiskLevel.MEDIUM
        else:
            return RiskLevel.LOW
    
    def _assess_availability_risk(self, files: List[FileAnalysis], 
                                batches: List[MigrationBatch]) -> RiskLevel:
        """Assess availability risk"""
        critical_files = sum(1 for f in files if f.category == FileCategory.CRITICAL)
        critical_ratio = critical_files / len(files) if files else 0
        
        max_batch_duration = max((b.estimated_duration for b in batches), default=0)
        
        if critical_ratio > 0.3 and max_batch_duration > 8:  # > 30% critical, > 8 hours
            return RiskLevel.HIGH
        elif critical_ratio > 0.1 and max_batch_duration > 4:  # > 10% critical, > 4 hours
            return RiskLevel.MEDIUM
        else:
            return RiskLevel.LOW
    
    def _assess_performance_risk(self, files: List[FileAnalysis]) -> RiskLevel:
        """Assess performance risk"""
        large_files = sum(1 for f in files if f.size > 1024**3)
        total_size = sum(f.size for f in files)
        
        if large_files > 100 or total_size > 10 * 1024**4:  # > 100 large files or > 10TB
            return RiskLevel.HIGH
        elif large_files > 10 or total_size > 1024**4:  # > 10 large files or > 1TB
            return RiskLevel.MEDIUM
        else:
            return RiskLevel.LOW
    
    def _assess_security_risk(self, files: List[FileAnalysis]) -> RiskLevel:
        """Assess security risk"""
        # This would typically involve checking file permissions, sensitive data, etc.
        # For now, return medium risk as a conservative estimate
        return RiskLevel.MEDIUM
    
    def _assess_operational_risk(self, batches: List[MigrationBatch]) -> RiskLevel:
        """Assess operational risk"""
        total_duration = sum(b.estimated_duration for b in batches)
        
        if total_duration > 72:  # > 72 hours
            return RiskLevel.HIGH
        elif total_duration > 24:  # > 24 hours
            return RiskLevel.MEDIUM
        else:
            return RiskLevel.LOW
    
    def _identify_specific_risks(self, files: List[FileAnalysis], 
                               batches: List[MigrationBatch]) -> List[Dict[str, Any]]:
        """Identify specific risks"""
        risks = []
        
        # Large file risks
        large_files = [f for f in files if f.size > 1024**3]
        if large_files:
            risks.append({
                'type': 'large_files',
                'description': f'{len(large_files)} files larger than 1GB may cause transfer timeouts',
                'severity': 'medium',
                'affected_files': len(large_files)
            })
        
        # Corrupted file risks
        corrupted_files = [f for f in files if f.is_corrupted]
        if corrupted_files:
            risks.append({
                'type': 'corrupted_files',
                'description': f'{len(corrupted_files)} files may be corrupted or inaccessible',
                'severity': 'high',
                'affected_files': len(corrupted_files)
            })
        
        # Capacity risks
        total_size_gb = sum(f.size for f in files) / 1024**3
        if total_size_gb > 1000:  # > 1TB
            risks.append({
                'type': 'storage_capacity',
                'description': f'Large total size ({total_size_gb:.1f}GB) may exceed storage limits',
                'severity': 'medium',
                'total_size_gb': total_size_gb
            })
        
        return risks
    
    def _generate_mitigation_strategies(self, risks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Generate mitigation strategies for identified risks"""
        strategies = []
        
        for risk in risks:
            if risk['type'] == 'large_files':
                strategies.append({
                    'risk_type': 'large_files',
                    'strategy': 'Implement chunked uploads for files >1GB',
                    'implementation': 'Use multipart upload with 100MB chunks',
                    'priority': 'high'
                })
            elif risk['type'] == 'corrupted_files':
                strategies.append({
                    'risk_type': 'corrupted_files',
                    'strategy': 'Pre-migration file validation and repair',
                    'implementation': 'Run integrity checks before migration starts',
                    'priority': 'critical'
                })
            elif risk['type'] == 'storage_capacity':
                strategies.append({
                    'risk_type': 'storage_capacity',
                    'strategy': 'Verify storage quotas and implement monitoring',
                    'implementation': 'Check R2 limits and set up capacity alerts',
                    'priority': 'medium'
                })
        
        return strategies
    
    def _format_size(self, size_bytes: int) -> str:
        """Format file size in human readable format"""
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.2f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.2f} PB"

class EnhancedMigrationAssessor:
    """Main enhanced migration assessment coordinator"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.file_analyzer = EnhancedFileAnalyzer(config)
        self.migration_planner = MigrationPlanner(config)
        
    async def assess_migration(self, django_media_root: str) -> EnhancedAssessmentReport:
        """Perform comprehensive migration assessment"""
        console.print("[bold green]ðŸŽ¯ Starting Enhanced Migration Assessment[/bold green]")
        
        assessment_id = f"assessment_{int(time.time())}"
        start_time = datetime.now(timezone.utc)
        
        try:
            # Phase 1: File system analysis
            console.print("\n[blue]Phase 1: Comprehensive File Analysis[/blue]")
            analysis_results = await self.file_analyzer.analyze_file_system(django_media_root)
            
            # Phase 2: Migration planning
            console.print("\n[blue]Phase 2: Migration Planning & Optimization[/blue]")
            migration_plan = self.migration_planner.generate_migration_plan(analysis_results)
            
            # Phase 3: Quality assessment
            console.print("\n[blue]Phase 3: Quality & Readiness Assessment[/blue]")
            quality_metrics = self._assess_quality_metrics(analysis_results)
            
            # Phase 4: Performance profiling
            console.print("\n[blue]Phase 4: Performance Profiling[/blue]")
            performance_profile = self._create_performance_profile(analysis_results)
            
            # Generate comprehensive report
            report = self._generate_comprehensive_report(
                assessment_id, start_time, analysis_results, 
                migration_plan, quality_metrics, performance_profile
            )
            
            console.print("\n[bold green]âœ… Enhanced Assessment Completed![/bold green]")
            return report
            
        except Exception as e:
            console.print(f"[red]âŒ Assessment failed: {e}[/red]")
            raise
    
    def _assess_quality_metrics(self, analysis_results: Dict[str, Any]) -> Dict[str, float]:
        """Assess data quality metrics"""
        files = analysis_results['files']
        
        if not files:
            return {'data_quality_score': 0.0, 'readiness_score': 0.0, 'complexity_score': 0.0}
        
        # Data quality score
        corrupted_files = sum(1 for f in files if f.is_corrupted)
        duplicate_files = sum(1 for f in files if f.is_duplicate)
        inaccessible_files = sum(1 for f in files if f.has_access_issues)
        
        quality_issues = corrupted_files + duplicate_files + inaccessible_files
        data_quality_score = max(0, 100 - (quality_issues / len(files) * 100))
        
        # Readiness score
        critical_files = sum(1 for f in files if f.category == FileCategory.CRITICAL)
        large_files = sum(1 for f in files if f.is_large_file)
        
        readiness_factors = [
            data_quality_score / 100,  # Quality factor
            1 - (critical_files / len(files)),  # Critical file factor
            1 - (large_files / len(files) * 0.5),  # Large file factor
        ]
        readiness_score = np.mean(readiness_factors) * 100
        
        # Complexity score
        total_size = sum(f.size for f in files)
        size_complexity = min(total_size / (1024**4), 1) * 30  # Up to 30 points for size
        file_count_complexity = min(len(files) / 100000, 1) * 30  # Up to 30 points for count
        type_diversity = len(set(f.extension for f in files)) / 100 * 40  # Up to 40 points for diversity
        
        complexity_score = size_complexity + file_count_complexity + type_diversity
        
        return {
            'data_quality_score': data_quality_score,
            'readiness_score': readiness_score,
            'complexity_score': complexity_score
        }
    
    def _create_performance_profile(self, analysis_results: Dict[str, Any]) -> PerformanceProfile:
        """Create performance profile from analysis data"""
        performance_data = analysis_results['performance_data']
        
        if not performance_data:
            return PerformanceProfile(
                avg_read_speed_mbps=0.0,
                peak_read_speed_mbps=0.0,
                avg_file_processing_time=0.0,
                large_file_processing_time=0.0,
                sequential_read_performance=0.0,
                random_read_performance=0.0,
                concurrent_read_performance=0.0
            )
        
        # Calculate read speeds
        read_speeds = []
        for data in performance_data:
            if data['read_time'] > 0:
                speed_mbps = (data['file_size'] / (1024 * 1024)) / data['read_time']
                read_speeds.append(speed_mbps)
        
        avg_read_speed = np.mean(read_speeds) if read_speeds else 0.0
        peak_read_speed = np.max(read_speeds) if read_speeds else 0.0
        
        # Processing times
        processing_times = [data['read_time'] + data.get('checksum_time', 0) for data in performance_data]
        avg_processing_time = np.mean(processing_times) if processing_times else 0.0
        
        # Large file processing times
        large_file_times = [
            data['read_time'] + data.get('checksum_time', 0) 
            for data in performance_data 
            if data['file_size'] > 1024**3
        ]
        large_file_processing_time = np.mean(large_file_times) if large_file_times else 0.0
        
        return PerformanceProfile(
            avg_read_speed_mbps=avg_read_speed,
            peak_read_speed_mbps=peak_read_speed,
            avg_file_processing_time=avg_processing_time,
            large_file_processing_time=large_file_processing_time,
            sequential_read_performance=avg_read_speed * 0.9,  # Estimated
            random_read_performance=avg_read_speed * 0.7,  # Estimated
            concurrent_read_performance=avg_read_speed * 1.2,  # Estimated
            identified_bottlenecks=["File system I/O", "Network bandwidth"],
            optimization_recommendations=[
                "Use SSD storage for better I/O performance",
                "Implement parallel processing for large files",
                "Optimize network configuration for sustained throughput"
            ]
        )
    
    def _generate_comprehensive_report(self, assessment_id: str, start_time: datetime,
                                     analysis_results: Dict[str, Any], 
                                     migration_plan: Dict[str, Any],
                                     quality_metrics: Dict[str, float],
                                     performance_profile: PerformanceProfile) -> EnhancedAssessmentReport:
        """Generate comprehensive assessment report"""
        files = analysis_results['files']
        
        # File categorization
        file_categories = defaultdict(int)
        for file in files:
            file_categories[file.category] += 1
        
        # File types
        file_types = defaultdict(int)
        for file in files:
            file_types[file.extension or 'no_extension'] += 1
        
        # Size distribution
        size_distribution = {
            'small (<1MB)': sum(1 for f in files if f.size < 1024**2),
            'medium (1MB-100MB)': sum(1 for f in files if 1024**2 <= f.size < 100*1024**2),
            'large (100MB-1GB)': sum(1 for f in files if 100*1024**2 <= f.size < 1024**3),
            'xlarge (>1GB)': sum(1 for f in files if f.size >= 1024**3)
        }
        
        # Access patterns
        access_patterns = dict(analysis_results['access_patterns'])
        
        # Generate recommendations
        recommendations = self._generate_recommendations(files, migration_plan, quality_metrics)
        action_items = self._generate_action_items(migration_plan['risk_assessment'])
        
        return EnhancedAssessmentReport(
            assessment_id=assessment_id,
            timestamp=start_time,
            total_files=len(files),
            total_size=sum(f.size for f in files),
            analyzed_files=len(files),
            file_categories=dict(file_categories),
            file_types=dict(file_types),
            size_distribution=size_distribution,
            access_patterns=access_patterns,
            risk_assessment=migration_plan['risk_assessment'],
            performance_profile=performance_profile,
            recommended_strategy=migration_plan['strategy'],
            migration_batches=migration_plan['batches'],
            infrastructure_requirements=migration_plan['infrastructure_requirements'],
            data_quality_score=quality_metrics['data_quality_score'],
            readiness_score=quality_metrics['readiness_score'],
            complexity_score=quality_metrics['complexity_score'],
            recommendations=recommendations,
            action_items=action_items
        )
    
    def _generate_recommendations(self, files: List[FileAnalysis], 
                                migration_plan: Dict[str, Any], 
                                quality_metrics: Dict[str, float]) -> List[str]:
        """Generate actionable recommendations"""
        recommendations = []
        
        # Data quality recommendations
        if quality_metrics['data_quality_score'] < 90:
            recommendations.append("Address data quality issues before migration")
        
        # Strategy recommendations
        if migration_plan['strategy'] == MigrationStrategy.PHASED:
            recommendations.append("Use phased migration approach to minimize risk")
        elif migration_plan['strategy'] == MigrationStrategy.PARALLEL:
            recommendations.append("Implement parallel processing for optimal performance")
        
        # Infrastructure recommendations
        infra = migration_plan['infrastructure_requirements']
        if infra.total_transfer_gb > 1000:
            recommendations.append("Consider dedicated network bandwidth for large migration")
        
        # Risk-based recommendations
        risk_level = migration_plan['risk_assessment'].overall_risk
        if risk_level in [RiskLevel.HIGH, RiskLevel.CRITICAL]:
            recommendations.append("Implement comprehensive testing and rollback procedures")
        
        return recommendations
    
    def _generate_action_items(self, risk_assessment: RiskAssessment) -> List[str]:
        """Generate specific action items"""
        action_items = []
        
        # Critical actions for high-risk migrations
        if risk_assessment.overall_risk in [RiskLevel.HIGH, RiskLevel.CRITICAL]:
            action_items.extend([
                "Create comprehensive backup before migration",
                "Establish rollback procedures and test them",
                "Set up real-time monitoring and alerting"
            ])
        
        # Standard actions
        action_items.extend([
            "Validate infrastructure requirements",
            "Test migration process with pilot batch",
            "Prepare migration runbook and procedures",
            "Schedule migration during low-traffic period"
        ])
        
        return action_items

# CLI Commands
@click.group()
@click.version_option(version="1.0.0")
def cli():
    """Enhanced Migration Assessment Tool"""
    pass

@cli.command()
@click.option('--media-root', required=True, help='Django media root directory')
@click.option('--output-file', help='Output file for assessment report')
@click.option('--format', 'output_format', type=click.Choice(['json', 'html']), 
              default='json', help='Output format')
@click.option('--batch-size', default=100, help='File analysis batch size')
@click.option('--max-workers', default=10, help='Maximum worker threads')
@click.option('--calculate-checksums', is_flag=True, help='Calculate file checksums')
@click.option('--detailed', is_flag=True, help='Include detailed analysis')
def assess(media_root, output_file, output_format, batch_size, max_workers, 
          calculate_checksums, detailed):
    """Perform enhanced migration assessment"""
    
    config = {
        'batch_size': batch_size,
        'max_workers': max_workers,
        'calculate_checksums': calculate_checksums,
        'detailed_analysis': detailed
    }
    
    assessor = EnhancedMigrationAssessor(config)
    
    try:
        # Run assessment
        report = asyncio.run(assessor.assess_migration(media_root))
        
        # Generate output
        if output_format == 'json':
            output_data = asdict(report)
            
            if output_file:
                with open(output_file, 'w') as f:
                    json.dump(output_data, f, indent=2, default=str)
                console.print(f"[green]Report saved to {output_file}[/green]")
            else:
                print(json.dumps(output_data, indent=2, default=str))
        
        # Display summary
        _display_assessment_summary(report)
        
    except Exception as e:
        console.print(f"[red]Assessment failed: {e}[/red]")
        sys.exit(1)

def _display_assessment_summary(report: EnhancedAssessmentReport):
    """Display assessment summary"""
    console.print("\n[bold blue]ðŸ“Š Assessment Summary[/bold blue]")
    
    # Basic statistics
    table = Table(title="Migration Overview")
    table.add_column("Metric", style="cyan")
    table.add_column("Value", style="green")
    
    table.add_row("Total Files", f"{report.total_files:,}")
    table.add_row("Total Size", f"{report.total_size / 1024**3:.2f} GB")
    table.add_row("Data Quality Score", f"{report.data_quality_score:.1f}%")
    table.add_row("Readiness Score", f"{report.readiness_score:.1f}%")
    table.add_row("Overall Readiness", report.overall_readiness())
    table.add_row("Recommended Strategy", report.recommended_strategy.value)
    table.add_row("Risk Level", report.risk_assessment.overall_risk.value)
    table.add_row("Estimated Duration", f"{report.infrastructure_requirements.estimated_duration_hours:.1f} hours")
    table.add_row("Estimated Cost", f"${report.infrastructure_requirements.total_cost:.2f}")
    
    console.print(table)
    
    # Migration batches
    if report.migration_batches:
        console.print(f"\n[bold]Migration Batches ({len(report.migration_batches)}):[/bold]")
        
        for i, batch in enumerate(report.migration_batches[:5], 1):
            console.print(f"  {i}. {batch.name} - {batch.total_files} files, "
                         f"{batch.total_size / 1024**3:.2f} GB, "
                         f"Risk: {batch.risk_level.value}")
        
        if len(report.migration_batches) > 5:
            console.print(f"  ... and {len(report.migration_batches) - 5} more batches")
    
    # Recommendations
    if report.recommendations:
        console.print(f"\n[bold]Key Recommendations:[/bold]")
        for rec in report.recommendations[:3]:
            console.print(f"  â€¢ {rec}")

if __name__ == '__main__':
    cli()